{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcFace + Swin\n",
    "- dataset, label, arcface parameter, modify\n",
    "- train, val, test, dfinitely split\n",
    "- W&B logging\n",
    "- pretrain = False, learning rates\n",
    "- bounding\n",
    "- ArcMarginProduct learned\n",
    "- Search Result\n",
    "- Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "\n",
    "import timm\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "from utils.utils import GaussianBlur , GeM\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_metric_learning import losses, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"ArcFace-DPat-SwinV2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find ArcFace-DPat-SwinV2.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmin1000\u001b[0m (\u001b[33mmin1k\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/v24684491/Swin_Arc_modified_TWCC/wandb/run-20240326_152329-e7si2zcx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/min1k/ArcFace-Swin/runs/e7si2zcx' target=\"_blank\">EffNetb5_Arc_0326_800768</a></strong> to <a href='https://wandb.ai/min1k/ArcFace-Swin' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/min1k/ArcFace-Swin' target=\"_blank\">https://wandb.ai/min1k/ArcFace-Swin</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/min1k/ArcFace-Swin/runs/e7si2zcx' target=\"_blank\">https://wandb.ai/min1k/ArcFace-Swin/runs/e7si2zcx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/min1k/ArcFace-Swin/runs/e7si2zcx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f31a7f32d10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"ArcFace-Swin\",\n",
    "           notes=\"EffNetb5_Arc_0326, augmentation with 800/768 resolution, random flip\",\n",
    "           name=\"EffNetb5_Arc_0326_800768\"\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gem_pool = GeM()\n",
    "\n",
    "class PatentNet(nn.Module):\n",
    "    def __init__(self, model_name: str, pretrained: bool, embedding_size: int, in_features):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, in_chans=1)\n",
    "\n",
    "        self.pool = gem_pool\n",
    "        self.head = nn.Sequential(\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(in_features, embedding_size),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_count = 33364 \n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "model = PatentNet(model_name='efficientnet_b5.sw_in12k_ft_in1k', pretrained=\"True\", embedding_size=512 , in_features = 2048).to(device)\n",
    "\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "log_interval = 5\n",
    "\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "scaler = GradScaler()\n",
    "def train(model, metric, loss_func, device, train_loader, optimizer, metric_optimizer, epoch, scheduler = None, metric_scheduler = None):\n",
    "    print(f\"Epoch {epoch} training start\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device) # labels\n",
    "        optimizer.zero_grad()\n",
    "        metric_optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            output = model(data)\n",
    "            output = metric(output,labels)\n",
    "            \n",
    "            loss = loss_func(output, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.step(metric_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        if batch_idx % 330 == 0:\n",
    "            print(\"Epoch {} Iteration {}: Train Loss = {}\".format(epoch, batch_idx, loss))\n",
    "        if batch_idx % log_interval == 0:\n",
    "            wandb.log({'epoch': epoch, 'train-iter': batch_idx, 'train-loss': loss})\n",
    "        \n",
    "        wandb.log({'lr': optimizer.param_groups[0]['lr'], 'metric_lr': metric_optimizer.param_groups[0]['lr']})\n",
    "        \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "        metric_scheduler.step()\n",
    "\n",
    "    total_loss = total_loss/len(train_loader)\n",
    "    wandb.log({'epoch': epoch, 'train-epoch-loss':total_loss})\n",
    "    print(\"End of epoch {}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "        test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "        train_labels = train_labels.squeeze(1)\n",
    "        # print(train_labels)\n",
    "        test_labels = test_labels.squeeze(1)\n",
    "        print(\"Computing accuracy\")\n",
    "        accuracies = accuracy_calculator.get_accuracy(\n",
    "            test_embeddings, test_labels, train_embeddings,  train_labels, False\n",
    "        )\n",
    "        prec_1 = accuracies[\"precision_at_1\"]\n",
    "        map = accuracies[\"mean_average_precision\"]\n",
    "        print(f\"Test set accuracy (Precision@1) = {prec_1}, (mAP) = {map}\")\n",
    "        wandb.log({'Prec@1': accuracies[\"precision_at_1\"], 'mAP': accuracies[\"mean_average_precision\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loss\n",
    "def test_calc(model, metric, loss_func, device, test_query_loader, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(test_query_loader):\n",
    "            data, labels = data.to(device), labels.to(device) # labels\n",
    "            embeddings = model(data)\n",
    "            output = metric(embeddings,labels)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\"Epoch {} Iteration {}: Test Loss = {}\".format(epoch, batch_idx, loss))\n",
    "            if batch_idx % log_interval == 0:\n",
    "                wandb.log({'epoch': epoch, 'test-iter': batch_idx, 'test-loss': loss})\n",
    "    \n",
    "        total_loss = total_loss/len(test_query_loader)\n",
    "        wandb.log({'epoch': epoch, 'test-epoch-loss':total_loss})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def valuation(val_db_set, val_query_set, model, accuracy_calculator):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_db_embeddings, val_db_labels = get_all_embeddings(val_db_set, model)\n",
    "        val_query_embeddings, val_query_labels = get_all_embeddings(val_query_set, model)\n",
    "        val_db_labels = val_db_labels.squeeze(1)\n",
    "        val_query_labels = val_query_labels.squeeze(1)\n",
    "        print(\"Val accuracy\")\n",
    "        accuracies = accuracy_calculator.get_accuracy(\n",
    "            val_query_embeddings, val_query_labels,val_db_embeddings, val_db_labels, False\n",
    "        )\n",
    "        prec_1 = accuracies[\"precision_at_1\"]\n",
    "        map = accuracies[\"mean_average_precision\"]\n",
    "        print(f\"Val set accuracy (Precision@1) = {prec_1}, (mAP) = {map}\")\n",
    "        wandb.log({'Val_Prec@1': accuracies[\"precision_at_1\"], 'Val_mAP': accuracies[\"mean_average_precision\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def valuation_calc(model, metric, loss_func, device, val_query_loader, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(val_query_loader):\n",
    "            data, labels = data.to(device), labels.to(device) # labels\n",
    "            embeddings = model(data)\n",
    "            output = metric(embeddings,labels)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\"Epoch {} Iteration {}: Test Loss = {}\".format(epoch, batch_idx, loss))\n",
    "            if batch_idx % log_interval == 0:\n",
    "                wandb.log({'epoch': epoch, 'val-iter': batch_idx, 'validation-loss': loss})\n",
    "        \n",
    "        total_loss = total_loss/len(val_query_loader)\n",
    "        wandb.log({'epoch': epoch, 'validation-epoch-loss':total_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform, dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize(size=(800,800)), \n",
    "        # transforms.RandomCrop((384,384), pad_if_needed=True, fill=255),\n",
    "        transforms.CenterCrop((768,768)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # transforms.RandomApply([GaussianBlur()], p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomErasing(p=0.5, scale=(0.22, 0.33), ratio=(0.3, 3.3), value=1, inplace=False), \n",
    "        # transforms.Normalize((0.5), (0.5)),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize(size=(800,800)),\n",
    "        transforms.CenterCrop((768,768)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5), (0.5)),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import glob\n",
    "def make_datapath_list(phase):\n",
    "\n",
    "    \n",
    "    rootpath = \"/work/v24684491/DeepPatent/patent_data/\"\n",
    "\n",
    "\n",
    "    if phase == \"train\":\n",
    "        with open('./patlist/train_patent_trn.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"test_query\":\n",
    "        with open('./patlist/test_query_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"test_db\":\n",
    "        with open('./patlist/test_db_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"val_query\":\n",
    "        with open('./patlist/val_query_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"val_db\":\n",
    "        with open('./patlist/val_db_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "    path_list = [] \n",
    "    for path in lines:\n",
    "        path = path.split()[0] # 0:path only, 1: label\n",
    "        path_list.append(rootpath+path)\n",
    "        \n",
    "    return path_list\n",
    "\n",
    "train_list = make_datapath_list(phase=\"train\")\n",
    "\n",
    "test_query_list = make_datapath_list(phase=\"test_query\")\n",
    "test_db_list = make_datapath_list(phase=\"test_db\")\n",
    "\n",
    "val_query_list = make_datapath_list(phase=\"val_query\")\n",
    "val_db_list = make_datapath_list(phase=\"val_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_list(phase):\n",
    "\n",
    "    \n",
    "    rootpath = \"/home/hmc/DeepPatent/patent_data/\"   # Enter the path to the dataset\n",
    "\n",
    "\n",
    "    if phase == \"train\":\n",
    "        with open('./patlist/train_patent_trn.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"test_query\":\n",
    "        with open('./patlist/test_query_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"test_db\":\n",
    "        with open('./patlist/test_db_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"val_query\":\n",
    "        with open('./patlist/val_query_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"val_db\":\n",
    "        with open('./patlist/val_db_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "    label_list = [] \n",
    "    for label in lines:\n",
    "        label = label.split()[1] # 0:path only, 1: label\n",
    "        label = int(label)\n",
    "        label_list.append(label)\n",
    "        \n",
    "    return label_list\n",
    "\n",
    "train_label_list = make_label_list(phase=\"train\")\n",
    "\n",
    "test_query_label_list = make_label_list(phase=\"test_query\")\n",
    "test_db_label_list = make_label_list(phase=\"test_db\")\n",
    "\n",
    "val_query_label_list = make_label_list(phase=\"val_query\")\n",
    "val_db_label_list = make_label_list(phase=\"val_db\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def cv2pil(image):\n",
    "\n",
    "    new_image = image.copy()\n",
    "    if new_image.ndim == 2:  \n",
    "        pass\n",
    "    elif new_image.shape[2] == 3:  \n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
    "    elif new_image.shape[2] == 4:  \n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_BGRA2RGBA)\n",
    "    new_image = Image.fromarray(new_image)\n",
    "    return new_image\n",
    "\n",
    "class PatentDataset(data.Dataset):\n",
    "    def __init__(self, file_list, label_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # open\n",
    "        _, img = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY)  \n",
    "\n",
    "        contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "        cnt = contours[0]\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(cnt) \n",
    "        img = img[y:y + h, x:x + w]  \n",
    "\n",
    "\n",
    "\n",
    "        img = cv2pil(img)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        label = self.label_list[index]\n",
    "        label = label - 1  # 0-indexed [0, nclass-1]\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_workers = 16\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PatentDataset(file_list=train_list, label_list=train_label_list, transform=data_transform['train'])\n",
    "\n",
    "test_query_data = PatentDataset(file_list=test_query_list, label_list=test_query_label_list, transform=data_transform['val'])\n",
    "test_db_data = PatentDataset(file_list=test_db_list, label_list=test_db_label_list, transform=data_transform['val'])\n",
    "\n",
    "val_query_data = PatentDataset(file_list=val_query_list, label_list=val_query_label_list, transform=data_transform['val'])\n",
    "val_db_data = PatentDataset(file_list=val_db_list, label_list=val_db_label_list, transform=data_transform['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "test_query_loader = torch.utils.data.DataLoader(test_query_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
    "test_db_loader = torch.utils.data.DataLoader(test_db_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "val_query_loader = torch.utils.data.DataLoader(val_query_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
    "val_db_loader = torch.utils.data.DataLoader(val_db_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses, distances, regularizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=64.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss, optimizer, accuracy setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "metric = ArcMarginProduct(512, app_count, s=20.0, m=0.50, easy_margin=True).to(device)\n",
    "metric_optimizer = optim.Adam(metric.parameters(), lr=5e-3)\n",
    "\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs )\n",
    "metric_scheduler = CosineAnnealingLR(metric_optimizer, T_max=num_epochs )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=1.5, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_func = FocalLoss()\n",
    "\n",
    "# loss_optimizer = optim.Adam(loss_func.parameters(), lr=1e-5) # from 1e-4\n",
    "\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\", \"mean_average_precision\"), k=\"max_bin_count\")\n",
    "### pytorch-metric-learning stuff ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GJ_L0TrTDnEA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "file_name = \"/work/v24684491/Saved_models/240326_EffNetb5_Arc_800768/\"\n",
    "if not os.path.exists(file_name):\n",
    "    os.makedirs(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9Kx_6EVLvA-"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "s1tQ_BWFLvA-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training start\n",
      "Epoch 1 Iteration 0: Train Loss = 16.085472106933594\n",
      "Epoch 1 Iteration 330: Train Loss = 16.093978881835938\n",
      "Epoch 1 Iteration 660: Train Loss = 13.462297439575195\n",
      "Epoch 1 Iteration 990: Train Loss = 13.14050006866455\n",
      "Epoch 1 Iteration 1320: Train Loss = 11.898300170898438\n",
      "Epoch 1 Iteration 1650: Train Loss = 10.906122207641602\n",
      "Epoch 1 Iteration 1980: Train Loss = 9.137556076049805\n",
      "End of epoch 1\n",
      "Epoch 1 training time cost 37.71 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [09:02<00:00,  1.93it/s]\n",
      "100%|██████████| 353/353 [02:57<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v24684491/.local/lib/python3.10/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set accuracy (Precision@1) = 0.9312394641114363, (mAP) = 0.8064360313449676\n",
      "Epoch 1 val time cost 12.12 mins\n",
      "Epoch 2 training start\n",
      "Epoch 2 Iteration 0: Train Loss = 7.656277179718018\n",
      "Epoch 2 Iteration 330: Train Loss = 7.708886623382568\n",
      "Epoch 2 Iteration 660: Train Loss = 7.918742656707764\n",
      "Epoch 2 Iteration 990: Train Loss = 7.749048709869385\n",
      "Epoch 2 Iteration 1320: Train Loss = 7.752127170562744\n",
      "Epoch 2 Iteration 1650: Train Loss = 6.998295783996582\n",
      "Epoch 2 Iteration 1980: Train Loss = 6.605541229248047\n",
      "End of epoch 2\n",
      "Epoch 2 training time cost 37.49 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [09:10<00:00,  1.90it/s]\n",
      "100%|██████████| 353/353 [03:04<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9505811374323484, (mAP) = 0.8678086299057918\n",
      "Epoch 2 val time cost 12.34 mins\n",
      "Epoch 3 training start\n",
      "Epoch 3 Iteration 0: Train Loss = 5.648679733276367\n",
      "Epoch 3 Iteration 330: Train Loss = 6.092467308044434\n",
      "Epoch 3 Iteration 660: Train Loss = 5.84442663192749\n",
      "Epoch 3 Iteration 990: Train Loss = 6.284336090087891\n",
      "Epoch 3 Iteration 1320: Train Loss = 5.3691511154174805\n",
      "Epoch 3 Iteration 1650: Train Loss = 5.426875114440918\n",
      "Epoch 3 Iteration 1980: Train Loss = 5.768845081329346\n",
      "End of epoch 3\n",
      "Epoch 3 training time cost 37.72 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:57<00:00,  1.95it/s]\n",
      "100%|██████████| 353/353 [03:01<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9586549551947475, (mAP) = 0.8883070111592087\n",
      "Epoch 3 val time cost 12.07 mins\n",
      "Epoch 4 training start\n",
      "Epoch 4 Iteration 0: Train Loss = 4.664182186126709\n",
      "Epoch 4 Iteration 330: Train Loss = 5.016646862030029\n",
      "Epoch 4 Iteration 660: Train Loss = 4.712860107421875\n",
      "Epoch 4 Iteration 990: Train Loss = 4.8282036781311035\n",
      "Epoch 4 Iteration 1320: Train Loss = 5.275130748748779\n",
      "Epoch 4 Iteration 1650: Train Loss = 4.948871612548828\n",
      "Epoch 4 Iteration 1980: Train Loss = 5.20021390914917\n",
      "End of epoch 4\n",
      "Epoch 4 training time cost 37.65 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:50<00:00,  1.98it/s]\n",
      "100%|██████████| 353/353 [03:04<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9640670747937183, (mAP) = 0.8983901749549976\n",
      "Epoch 4 val time cost 11.99 mins\n",
      "Epoch 5 training start\n",
      "Epoch 5 Iteration 0: Train Loss = 4.029406547546387\n",
      "Epoch 5 Iteration 330: Train Loss = 4.13071870803833\n",
      "Epoch 5 Iteration 660: Train Loss = 4.204578876495361\n",
      "Epoch 5 Iteration 990: Train Loss = 4.550624847412109\n",
      "Epoch 5 Iteration 1320: Train Loss = 4.284197807312012\n",
      "Epoch 5 Iteration 1650: Train Loss = 3.473954200744629\n",
      "Epoch 5 Iteration 1980: Train Loss = 4.3349609375\n",
      "End of epoch 5\n",
      "Epoch 5 training time cost 37.97 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:58<00:00,  1.95it/s]\n",
      "100%|██████████| 353/353 [03:03<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9666400496850324, (mAP) = 0.9074497264606739\n",
      "Epoch 5 val time cost 12.12 mins\n",
      "Epoch 6 training start\n",
      "Epoch 6 Iteration 0: Train Loss = 3.4200453758239746\n",
      "Epoch 6 Iteration 330: Train Loss = 2.693394422531128\n",
      "Epoch 6 Iteration 660: Train Loss = 3.624556303024292\n",
      "Epoch 6 Iteration 990: Train Loss = 3.9459052085876465\n",
      "Epoch 6 Iteration 1320: Train Loss = 3.9600510597229004\n",
      "Epoch 6 Iteration 1650: Train Loss = 3.904332160949707\n",
      "Epoch 6 Iteration 1980: Train Loss = 3.516683578491211\n",
      "End of epoch 6\n",
      "Epoch 6 training time cost 37.74 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [09:01<00:00,  1.94it/s]\n",
      "100%|██████████| 353/353 [03:03<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9663738798686895, (mAP) = 0.9095121353122732\n",
      "Epoch 6 val time cost 12.16 mins\n",
      "Epoch 7 training start\n",
      "Epoch 7 Iteration 0: Train Loss = 3.038846492767334\n",
      "Epoch 7 Iteration 330: Train Loss = 2.931914806365967\n",
      "Epoch 7 Iteration 660: Train Loss = 2.9674408435821533\n",
      "Epoch 7 Iteration 990: Train Loss = 3.278698682785034\n",
      "Epoch 7 Iteration 1320: Train Loss = 2.965907096862793\n",
      "Epoch 7 Iteration 1650: Train Loss = 3.301048994064331\n",
      "Epoch 7 Iteration 1980: Train Loss = 2.8718132972717285\n",
      "End of epoch 7\n",
      "Epoch 7 training time cost 37.64 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:54<00:00,  1.96it/s]\n",
      "100%|██████████| 353/353 [03:02<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.968769408215775, (mAP) = 0.9131598617298922\n",
      "Epoch 7 val time cost 12.03 mins\n",
      "Epoch 8 training start\n",
      "Epoch 8 Iteration 0: Train Loss = 2.460202693939209\n",
      "Epoch 8 Iteration 330: Train Loss = 2.400390625\n",
      "Epoch 8 Iteration 660: Train Loss = 2.49562668800354\n",
      "Epoch 8 Iteration 990: Train Loss = 2.645988702774048\n",
      "Epoch 8 Iteration 1320: Train Loss = 2.3652658462524414\n",
      "Epoch 8 Iteration 1650: Train Loss = 2.436467170715332\n",
      "Epoch 8 Iteration 1980: Train Loss = 2.391643524169922\n",
      "End of epoch 8\n",
      "Epoch 8 training time cost 37.38 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:56<00:00,  1.96it/s]\n",
      "100%|██████████| 353/353 [03:05<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.968769408215775, (mAP) = 0.9119612036273137\n",
      "Epoch 8 val time cost 12.11 mins\n",
      "Epoch 9 training start\n",
      "Epoch 9 Iteration 0: Train Loss = 1.5951472520828247\n",
      "Epoch 9 Iteration 330: Train Loss = 2.0219547748565674\n",
      "Epoch 9 Iteration 660: Train Loss = 2.0009548664093018\n",
      "Epoch 9 Iteration 990: Train Loss = 2.1911308765411377\n",
      "Epoch 9 Iteration 1320: Train Loss = 2.646219491958618\n",
      "Epoch 9 Iteration 1650: Train Loss = 1.8648300170898438\n",
      "Epoch 9 Iteration 1980: Train Loss = 2.0022594928741455\n",
      "End of epoch 9\n",
      "Epoch 9 training time cost 37.4 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:57<00:00,  1.95it/s]\n",
      "100%|██████████| 353/353 [03:00<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9674385591340607, (mAP) = 0.9109142126092202\n",
      "Epoch 9 val time cost 12.04 mins\n",
      "Epoch 10 training start\n",
      "Epoch 10 Iteration 0: Train Loss = 1.2097231149673462\n",
      "Epoch 10 Iteration 330: Train Loss = 1.9260237216949463\n",
      "Epoch 10 Iteration 660: Train Loss = 1.562468409538269\n",
      "Epoch 10 Iteration 990: Train Loss = 2.005345582962036\n",
      "Epoch 10 Iteration 1320: Train Loss = 1.5996906757354736\n",
      "Epoch 10 Iteration 1650: Train Loss = 1.7065647840499878\n",
      "Epoch 10 Iteration 1980: Train Loss = 1.9262206554412842\n",
      "End of epoch 10\n",
      "Epoch 10 training time cost 37.42 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:50<00:00,  1.98it/s]\n",
      "100%|██████████| 353/353 [03:03<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9692130245763463, (mAP) = 0.9115442074933608\n",
      "Epoch 10 val time cost 11.99 mins\n",
      "Epoch 11 training start\n",
      "Epoch 11 Iteration 0: Train Loss = 0.9912635684013367\n",
      "Epoch 11 Iteration 330: Train Loss = 1.3493074178695679\n",
      "Epoch 11 Iteration 660: Train Loss = 1.3699525594711304\n",
      "Epoch 11 Iteration 990: Train Loss = 1.3384138345718384\n",
      "Epoch 11 Iteration 1320: Train Loss = 1.4601702690124512\n",
      "Epoch 11 Iteration 1650: Train Loss = 1.3882545232772827\n",
      "Epoch 11 Iteration 1980: Train Loss = 1.1104894876480103\n",
      "End of epoch 11\n",
      "Epoch 11 training time cost 37.51 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:40<00:00,  2.01it/s]\n",
      "100%|██████████| 353/353 [02:57<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9683257918552035, (mAP) = 0.9115643472968701\n",
      "Epoch 11 val time cost 11.72 mins\n",
      "Epoch 12 training start\n",
      "Epoch 12 Iteration 0: Train Loss = 1.0276392698287964\n",
      "Epoch 12 Iteration 330: Train Loss = 0.7759753465652466\n",
      "Epoch 12 Iteration 660: Train Loss = 1.379970908164978\n",
      "Epoch 12 Iteration 990: Train Loss = 1.2826577425003052\n",
      "Epoch 12 Iteration 1320: Train Loss = 0.8586421608924866\n",
      "Epoch 12 Iteration 1650: Train Loss = 1.119847297668457\n",
      "Epoch 12 Iteration 1980: Train Loss = 1.1675729751586914\n",
      "End of epoch 12\n",
      "Epoch 12 training time cost 37.08 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [09:06<00:00,  1.92it/s]\n",
      "100%|██████████| 353/353 [03:04<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.968769408215775, (mAP) = 0.9097352207016727\n",
      "Epoch 12 val time cost 12.27 mins\n",
      "Epoch 13 training start\n",
      "Epoch 13 Iteration 0: Train Loss = 0.7927025556564331\n",
      "Epoch 13 Iteration 330: Train Loss = 0.8356321454048157\n",
      "Epoch 13 Iteration 660: Train Loss = 0.859424352645874\n",
      "Epoch 13 Iteration 990: Train Loss = 1.0186045169830322\n",
      "Epoch 13 Iteration 1320: Train Loss = 1.2137207984924316\n",
      "Epoch 13 Iteration 1650: Train Loss = 0.9248602986335754\n",
      "Epoch 13 Iteration 1980: Train Loss = 1.280200719833374\n",
      "End of epoch 13\n",
      "Epoch 13 training time cost 37.26 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [08:49<00:00,  1.98it/s]\n",
      "100%|██████████| 353/353 [03:05<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9677934522225179, (mAP) = 0.9106657455803439\n",
      "Epoch 13 val time cost 11.99 mins\n",
      "Epoch 14 training start\n",
      "Epoch 14 Iteration 0: Train Loss = 0.7744786143302917\n",
      "Epoch 14 Iteration 330: Train Loss = 0.8074260950088501\n",
      "Epoch 14 Iteration 660: Train Loss = 0.7945393919944763\n",
      "Epoch 14 Iteration 990: Train Loss = 0.6503770351409912\n",
      "Epoch 14 Iteration 1320: Train Loss = 0.7111985087394714\n",
      "Epoch 14 Iteration 1650: Train Loss = 0.8988957405090332\n",
      "Epoch 14 Iteration 1980: Train Loss = 0.9151913523674011\n",
      "End of epoch 14\n",
      "Epoch 14 training time cost 37.05 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [09:17<00:00,  1.88it/s]\n",
      "100%|██████████| 353/353 [03:09<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9685919616715464, (mAP) = 0.9109155832677212\n",
      "Epoch 14 val time cost 12.55 mins\n",
      "Epoch 15 training start\n",
      "Epoch 15 Iteration 0: Train Loss = 0.9531362652778625\n",
      "Epoch 15 Iteration 330: Train Loss = 1.0061933994293213\n",
      "Epoch 15 Iteration 660: Train Loss = 0.6006003022193909\n",
      "Epoch 15 Iteration 990: Train Loss = 0.9859896898269653\n",
      "Epoch 15 Iteration 1320: Train Loss = 0.618428111076355\n",
      "Epoch 15 Iteration 1650: Train Loss = 0.8193310499191284\n",
      "Epoch 15 Iteration 1980: Train Loss = 0.8333116173744202\n",
      "End of epoch 15\n",
      "Epoch 15 training time cost 37.12 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049/1049 [10:45<00:00,  1.62it/s]\n",
      "100%|██████████| 353/353 [03:39<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy\n",
      "Val set accuracy (Precision@1) = 0.9695679176648034, (mAP) = 0.9121211905455285\n",
      "Epoch 15 val time cost 14.51 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [12:46<00:00,  1.58it/s]\n",
      "100%|██████████| 411/411 [04:23<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9646691540394426, (mAP) = 0.9034256129829507\n",
      "Epoch 15 test time cost 17.21 mins\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Prec@1</td><td>▁</td></tr><tr><td>Val_Prec@1</td><td>▁▅▆▇▇▇█████████</td></tr><tr><td>Val_mAP</td><td>▁▅▆▇███████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>████████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>mAP</td><td>▁</td></tr><tr><td>metric_lr</td><td>████████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train-epoch-loss</td><td>█▅▄▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train-iter</td><td>▂▄▇▃▅█▄▆▁▅▇▂▆█▄▇▂▅▇▃▆▁▃▇▂▄█▃▆▂▄▇▂▅▁▃▆▂▄█</td></tr><tr><td>train-loss</td><td>█▇▆▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Prec@1</td><td>0.96467</td></tr><tr><td>Val_Prec@1</td><td>0.96957</td></tr><tr><td>Val_mAP</td><td>0.91212</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>mAP</td><td>0.90343</td></tr><tr><td>metric_lr</td><td>5e-05</td></tr><tr><td>train-epoch-loss</td><td>0.8116</td></tr><tr><td>train-iter</td><td>1985</td></tr><tr><td>train-loss</td><td>0.56736</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EffNetb5_Arc_0326_800768</strong> at: <a href='https://wandb.ai/min1k/ArcFace-Swin/runs/e7si2zcx' target=\"_blank\">https://wandb.ai/min1k/ArcFace-Swin/runs/e7si2zcx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_152329-e7si2zcx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    time_start = time.time()\n",
    "#     train(model, loss_func, device, train_loader, optimizer, loss_optimizer, epoch)\n",
    "    train(model, metric, loss_func, device, train_loader, optimizer, metric_optimizer, epoch, scheduler, metric_scheduler)\n",
    "#     train(model, metric, loss_func, device, train_loader, optimizer, metric_optimizer, epoch)\n",
    "    \n",
    "    torch.save({'state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            \"metric_optimizer_dict\" : metric_optimizer.state_dict(),\n",
    "            \"metric_dict\" : metric.state_dict(),\n",
    "            \"lr_scheduler_dict\" : scheduler.state_dict(),\n",
    "            \"metric_lr_scheduler_dict\" : metric_scheduler.state_dict(),\n",
    "            'epochs' : epoch }, file_name +'Swinv2_Arc{}.pth'.format(epoch))\n",
    "    \n",
    "    # torch.save({\"state_dict\":model.state_dict(),\"epoch\":epoch}, file_name +'EffArc_Gem-test_epoch{}.pth'.format(epoch))\n",
    "    \n",
    "    time_end = time.time()   \n",
    "    time_c= time_end - time_start   \n",
    "    print(f'Epoch {epoch} training time cost {round(time_c/60,2)} mins') \n",
    "    \n",
    "    valuation(val_db_data, val_query_data, model, accuracy_calculator)\n",
    "    # valuation_calc(model, metric, loss_func, device, val_query_loader, epoch)\n",
    "\n",
    "    val_time_end = time.time()    \n",
    "    time_c= val_time_end - time_end   \n",
    "    print(f'Epoch {epoch} val time cost {round(time_c/60,2)} mins')\n",
    "    \n",
    "    if epoch == num_epochs:\n",
    "        test(test_db_data, test_query_data, model, accuracy_calculator)\n",
    "        # test_calc(model, metric, loss_func, device, test_query_loader, epoch)\n",
    "\n",
    "        test_time_end = time.time()    \n",
    "        time_c= test_time_end - val_time_end   \n",
    "        print(f'Epoch {epoch} test time cost {round(time_c/60,2)} mins')\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SubCenterArcFaceMNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
