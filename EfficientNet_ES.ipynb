{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcFace + Swin\n",
    "- dataset, label, arcface parameter, modify\n",
    "- train, val, test, dfinitely split\n",
    "- W&B logging\n",
    "- pretrain = False, learning rates\n",
    "- bounding\n",
    "- ArcMarginProduct learned\n",
    "- Search Result\n",
    "- Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1712723662.642781] [w1qeblctr1712723101201-fvttg:125  :f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "# import torchvision\n",
    "import timm\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "from utils.utils import GaussianBlur , GeM\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# from models.swintrans import PatentNet\n",
    "# from models.swin_transformer_v2 import swinv2_base_window12to24_192to384\n",
    "\n",
    "from pytorch_metric_learning import losses, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"http_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "# os.environ[\"https_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"ArcFace-DPat-SwinV2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find ArcFace-DPat-SwinV2.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmin1000\u001b[0m (\u001b[33mmin1k\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/v24684491/Swin_Arc_modified_TWCC/wandb/run-20240410_123434-3k9igoww</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/min1k/ArcFace-Swin/runs/3k9igoww' target=\"_blank\">EffNetb5_Arc_0410_pretrainmodel_on_dbscan_crop</a></strong> to <a href='https://wandb.ai/min1k/ArcFace-Swin' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/min1k/ArcFace-Swin' target=\"_blank\">https://wandb.ai/min1k/ArcFace-Swin</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/min1k/ArcFace-Swin/runs/3k9igoww' target=\"_blank\">https://wandb.ai/min1k/ArcFace-Swin/runs/3k9igoww</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/min1k/ArcFace-Swin/runs/3k9igoww?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdb09c6ba00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"ArcFace-Swin\",\n",
    "           notes=\"EffNetb5預訓練模型繼續訓練在新資料集\",\n",
    "           name=\"EffNetb5_Arc_0410_pretrainmodel_on_dbscan_crop\"\n",
    "           )\n",
    "# wandb.finish()240410_Swinv2_Arc_dbscan_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gem_pool = GeM()\n",
    "\n",
    "class PatentNet(nn.Module):\n",
    "    def __init__(self, model_name: str, pretrained: bool, embedding_size: int, in_features):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, in_chans=1, global_pool=\"\")\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, in_chans=1)\n",
    "        # in_features = 1280 # efficentnet_b0\n",
    "        # in_features = in_features\n",
    "        \n",
    "#         in_features = 1280 # efnet\n",
    "#         in_features = 2048 # resnet50\n",
    "#         in_features = 768 # vit_b_16\n",
    "        \n",
    "        # in_features = 1024 # swin_base\n",
    "        self.pool = gem_pool\n",
    "        self.head = nn.Sequential(\n",
    "        #     nn.Linear(in_features=1024, out_features=1000, bias=True),\n",
    "            nn.Flatten(),\n",
    "        #     nn.BatchNorm1d(in_features),\n",
    "        #     nn.Dropout(),\n",
    "            nn.Linear(in_features, embedding_size),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "        #     nn.PReLU(),\n",
    "        )\n",
    "#         self.arcface = ArcMarginProduct(embedding_size,class_num)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "#         x = self.pool(x) # 2D\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_count = 33364 # 出力クラス数\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# model = PatentNet(model_name=\"swinv2_base_window12to24_192to384.ms_in22k_ft_in1k\", pretrained=\"True\", embedding_size=512, in_features = 1024).to(device) # 実験でfalse変更\n",
    "# model = PatentNet(model_name='efficientnet_b4', pretrained=\"True\", embedding_size=512 , in_features = 1792).to(device)\n",
    "model = PatentNet(model_name='efficientnet_b5.sw_in12k_ft_in1k', pretrained=\"True\", embedding_size=512 , in_features = 2048).to(device)\n",
    "# model = PatentNet(model_name=\"resnet50\", pretrained=\"True\", embedding_size=1000).to(device)\n",
    "tmp = torch.load(\"/work/v24684491/Saved_models/240326_EffNetb5_Arc_800768/Swinv2_Arc7.pth\")\n",
    "model.load_state_dict(tmp['state_dict'])\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# batch_size = 8\n",
    "log_interval = 5\n",
    "\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "scaler = GradScaler()\n",
    "def train(model, metric, loss_func, device, train_loader, optimizer, metric_optimizer, epoch, scheduler = None, metric_scheduler = None):\n",
    "    print(f\"Epoch {epoch} training start\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device) # labels\n",
    "        optimizer.zero_grad()\n",
    "        metric_optimizer.zero_grad()\n",
    "        # output = model(data)\n",
    "        # output = metric(output,labels)\n",
    "        # loss = loss_func(output, labels)\n",
    "        # loss.backward()\n",
    "\n",
    "        # optimizer.step()\n",
    "        # metric_optimizer.step()\n",
    "        \n",
    "        with autocast():\n",
    "            output = model(data)\n",
    "            output = metric(output,labels)\n",
    "            \n",
    "            loss = loss_func(output, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.step(metric_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        if batch_idx % 330 == 0:\n",
    "            print(\"Epoch {} Iteration {}: Train Loss = {}\".format(epoch, batch_idx, loss))\n",
    "        if batch_idx % log_interval == 0:\n",
    "            wandb.log({'epoch': epoch, 'train-iter': batch_idx, 'train-loss': loss})\n",
    "        \n",
    "        wandb.log({'lr': optimizer.param_groups[0]['lr'], 'metric_lr': metric_optimizer.param_groups[0]['lr']})\n",
    "        \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "        metric_scheduler.step()\n",
    "\n",
    "    total_loss = total_loss/len(train_loader)\n",
    "    wandb.log({'epoch': epoch, 'train-epoch-loss':total_loss})\n",
    "    print(\"End of epoch {}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "        test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "        train_labels = train_labels.squeeze(1)\n",
    "        # print(train_labels)\n",
    "        test_labels = test_labels.squeeze(1)\n",
    "        print(\"Computing accuracy\")\n",
    "        accuracies = accuracy_calculator.get_accuracy(\n",
    "            test_embeddings, test_labels, train_embeddings,  train_labels, False\n",
    "        )\n",
    "        prec_1 = accuracies[\"precision_at_1\"]\n",
    "        map = accuracies[\"mean_average_precision\"]\n",
    "        print(f\"Test set accuracy (Precision@1) = {prec_1}, (mAP) = {map}\")\n",
    "        wandb.log({'Prec@1': accuracies[\"precision_at_1\"], 'mAP': accuracies[\"mean_average_precision\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算測試損失\n",
    "def test_calc(model, metric, loss_func, device, test_query_loader, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(test_query_loader):\n",
    "            data, labels = data.to(device), labels.to(device) # labels\n",
    "            embeddings = model(data)\n",
    "            output = metric(embeddings,labels)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\"Epoch {} Iteration {}: Test Loss = {}\".format(epoch, batch_idx, loss))\n",
    "            if batch_idx % log_interval == 0:\n",
    "                wandb.log({'epoch': epoch, 'test-iter': batch_idx, 'test-loss': loss})\n",
    "    \n",
    "        total_loss = total_loss/len(test_query_loader)\n",
    "        wandb.log({'epoch': epoch, 'test-epoch-loss':total_loss})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def valuation(val_db_set, val_query_set, model, accuracy_calculator):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_db_embeddings, val_db_labels = get_all_embeddings(val_db_set, model)\n",
    "        val_query_embeddings, val_query_labels = get_all_embeddings(val_query_set, model)\n",
    "        val_db_labels = val_db_labels.squeeze(1)\n",
    "        val_query_labels = val_query_labels.squeeze(1)\n",
    "        print(\"Val accuracy\")\n",
    "        accuracies = accuracy_calculator.get_accuracy(\n",
    "            val_query_embeddings, val_query_labels,val_db_embeddings, val_db_labels, False\n",
    "        )\n",
    "        prec_1 = accuracies[\"precision_at_1\"]\n",
    "        map = accuracies[\"mean_average_precision\"]\n",
    "        print(f\"Val set accuracy (Precision@1) = {prec_1}, (mAP) = {map}\")\n",
    "        wandb.log({'Val_Prec@1': accuracies[\"precision_at_1\"], 'Val_mAP': accuracies[\"mean_average_precision\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val損失の計算\n",
    "def valuation_calc(model, metric, loss_func, device, val_query_loader, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(val_query_loader):\n",
    "            data, labels = data.to(device), labels.to(device) # labels\n",
    "            embeddings = model(data)\n",
    "            output = metric(embeddings,labels)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\"Epoch {} Iteration {}: Test Loss = {}\".format(epoch, batch_idx, loss))\n",
    "            if batch_idx % log_interval == 0:\n",
    "                wandb.log({'epoch': epoch, 'val-iter': batch_idx, 'validation-loss': loss})\n",
    "        \n",
    "        total_loss = total_loss/len(val_query_loader)\n",
    "        wandb.log({'epoch': epoch, 'validation-epoch-loss':total_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform, dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((768,768)), # 短辺384\n",
    "        # transforms.RandomCrop((384,384), pad_if_needed=True, fill=255),\n",
    "        # transforms.CenterCrop((768,768)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # transforms.RandomApply([GaussianBlur()], p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomErasing(p=0.5, scale=(0.22, 0.33), ratio=(0.3, 3.3), value=1, inplace=False), # ToTensorの後\n",
    "        # transforms.Normalize((0.5), (0.5)),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize(size=(768,768)),\n",
    "        # transforms.CenterCrop((768,768)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5), (0.5)),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasetを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import glob\n",
    "def make_datapath_list(phase):\n",
    "    \"\"\"\n",
    "    phase: train or val\n",
    "    path_list: データパスのリスト\n",
    "    \"\"\"\n",
    "    \n",
    "    rootpath = \"/work/v24684491/DeepPatent/patent_data/\"\n",
    "#     rootpath = \"/host/space0/higuchi-k/dpatent_dataset/patent_data/\"\n",
    "\n",
    "    if phase == \"train\":\n",
    "        with open('./patlist/17_train_patent_trn_rebuild_2percentageup_sorted.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"test_query\":\n",
    "        with open('./patlist/test_query_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"test_db\":\n",
    "        with open('./patlist/test_db_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"val_query\":\n",
    "        with open('./patlist/val_query_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"val_db\":\n",
    "        with open('./patlist/val_db_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "    path_list = [] # ここに格納\n",
    "    for path in lines:\n",
    "        path = path.split()[0] # 0:path only, 1: label\n",
    "        path_list.append(rootpath+path)\n",
    "        \n",
    "    return path_list\n",
    "\n",
    "train_list = make_datapath_list(phase=\"train\")\n",
    "\n",
    "test_query_list = make_datapath_list(phase=\"test_query\")\n",
    "test_db_list = make_datapath_list(phase=\"test_db\")\n",
    "\n",
    "val_query_list = make_datapath_list(phase=\"val_query\")\n",
    "val_db_list = make_datapath_list(phase=\"val_db\")\n",
    "\n",
    "# print(train_list[0:3] , \"\\n\")\n",
    "# print(\"train_list:\", len(train_list), \"\\n\")\n",
    "# print(\"test_query_list:\", len(test_query_list))\n",
    "# print(\"test_db_list:\", len(test_db_list), \"\\n\")\n",
    "# print(\"val_query_list:\", len(val_query_list))\n",
    "# print(\"val_db_list\", len(val_db_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_list(phase):\n",
    "    \"\"\"\n",
    "    phase: train or val\n",
    "    label_list: データラベルのリスト\n",
    "    \"\"\"\n",
    "    \n",
    "    rootpath = \"/home/hmc/DeepPatent/patent_data/\"\n",
    "#     rootpath = \"/host/space0/higuchi-k/dpatent_dataset/patent_data/\"\n",
    "\n",
    "    if phase == \"train\":\n",
    "        with open('./patlist/17_train_patent_trn_rebuild_2percentageup_sorted.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"test_query\":\n",
    "        with open('./patlist/test_query_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"test_db\":\n",
    "        with open('./patlist/test_db_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"val_query\":\n",
    "        with open('./patlist/val_query_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "    elif phase == \"val_db\":\n",
    "        with open('./patlist/val_db_patent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "    label_list = [] # ここに格納\n",
    "    for label in lines:\n",
    "        label = label.split()[1] # 0:path only, 1: label\n",
    "        label = int(label)\n",
    "        label_list.append(label)\n",
    "        \n",
    "    return label_list\n",
    "\n",
    "train_label_list = make_label_list(phase=\"train\")\n",
    "\n",
    "test_query_label_list = make_label_list(phase=\"test_query\")\n",
    "test_db_label_list = make_label_list(phase=\"test_db\")\n",
    "\n",
    "val_query_label_list = make_label_list(phase=\"val_query\")\n",
    "val_db_label_list = make_label_list(phase=\"val_db\")\n",
    "\n",
    "# print(train_label_list[0:3])\n",
    "# print(type(train_label_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def cv2pil(image):\n",
    "    ''' OpenCV型 -> PIL型 '''\n",
    "    new_image = image.copy()\n",
    "    if new_image.ndim == 2:  # 單色\n",
    "        pass\n",
    "    elif new_image.shape[2] == 3:  # 彩色\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
    "    elif new_image.shape[2] == 4:  # 透過\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_BGRA2RGBA)\n",
    "    new_image = Image.fromarray(new_image)\n",
    "    return new_image\n",
    "\n",
    "class PatentDataset(data.Dataset):\n",
    "    def __init__(self, file_list, label_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # open\n",
    "        _, img = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY)  # 二値化(白底為1，黑線為0)\n",
    "\n",
    "        contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # 取得輪廓\n",
    "        try:\n",
    "            cnt = contours[0]\n",
    "        except: \n",
    "            print(f\"Error from {img_path}\")\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(cnt)  # 取得外接矩形\n",
    "        img = img[y:y + h, x:x + w]  # 剪出外接矩形\n",
    "\n",
    "        # 计算 padding 大小\n",
    "        # max_side = max(w, h)\n",
    "        # pad_x = (max_side - w) // 2\n",
    "        # pad_y = (max_side - h) // 2\n",
    "\n",
    "        # 进行 padding\n",
    "        # img = cv2.copyMakeBorder(img, pad_y, pad_y, pad_x, pad_x, cv2.BORDER_CONSTANT, value=255)\n",
    "\n",
    "        # _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)  # 反二値化(白底為0，黑線為1)\n",
    "\n",
    "        img = cv2pil(img)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        label = self.label_list[index]\n",
    "        label = label - 1  # 0-indexed [0, nclass-1]轉換為標籤\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# num_workers = os.cpu_count() - 1\n",
    "num_workers = 16\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PatentDataset(file_list=train_list, label_list=train_label_list, transform=data_transform['train'])\n",
    "\n",
    "test_query_data = PatentDataset(file_list=test_query_list, label_list=test_query_label_list, transform=data_transform['val'])\n",
    "test_db_data = PatentDataset(file_list=test_db_list, label_list=test_db_label_list, transform=data_transform['val'])\n",
    "\n",
    "val_query_data = PatentDataset(file_list=val_query_list, label_list=val_query_label_list, transform=data_transform['val'])\n",
    "val_db_data = PatentDataset(file_list=val_db_list, label_list=val_db_label_list, transform=data_transform['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "test_query_loader = torch.utils.data.DataLoader(test_query_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
    "test_db_loader = torch.utils.data.DataLoader(test_db_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "val_query_loader = torch.utils.data.DataLoader(val_query_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
    "val_db_loader = torch.utils.data.DataLoader(val_db_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses, distances, regularizers\n",
    "\n",
    "# distance = distances.CosineSimilarity()\n",
    "# regularizer = regularizers.RegularFaceRegularizer()\n",
    "# sampler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=64.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss, optimizer, accuracy設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=scheduler_params['lr_start'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "metric = ArcMarginProduct(512, app_count, s=20.0, m=0.50, easy_margin=True).to(device)\n",
    "metric_optimizer = optim.Adam(metric.parameters(), lr=5e-3)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "# metric_optimizer = optim.SGD(metric.parameters(), lr=5e-3)\n",
    "\n",
    "# Defining LR SCheduler\n",
    "# scheduler = JPOScheduler(optimizer, **scheduler_params)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs )\n",
    "metric_scheduler = CosineAnnealingLR(metric_optimizer, T_max=num_epochs )\n",
    "\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "# metric_scheduler = CosineAnnealingWarmRestarts(metric_optimizer, T_0=1, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma = 2.0 変更\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=1.5, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pytorch-metric-learning stuff ###\n",
    "\n",
    "# loss_func = losses.ArcFaceLoss(num_classes=app_count, embedding_size=1000, margin=28.6, scale=64, weight_regularizer=regularizer, distance=distance).to(device)\n",
    "# loss_func = losses.ArcFaceLoss(num_classes=app_count, embedding_size=1000, margin=0.1, scale=8, weight_regularizer=regularizer, distance=distance).to(device)\n",
    "# loss_func = losses.SubCenterArcFaceLoss(num_classes=app_count, embedding_size=1000, margin=28.6, scale=64, sub_centers=3, weight_regularizer=regularizer, distance=distance).to(device)\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "loss_func = FocalLoss()\n",
    "\n",
    "# loss_optimizer = optim.Adam(loss_func.parameters(), lr=1e-5) # from 1e-4\n",
    "\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\", \"mean_average_precision\"), k=\"max_bin_count\")\n",
    "### pytorch-metric-learning stuff ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GJ_L0TrTDnEA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "file_name = \"/work/v24684491/Saved_models/240410_EffNetb5_Arc_dbscan_train/\"\n",
    "if not os.path.exists(file_name):\n",
    "    os.makedirs(file_name)\n",
    "# print(file_name + 'eff-arcface-test.pth')\n",
    "# torch.save(model.state_dict(), file_name +'Swinv2_Arc.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9Kx_6EVLvA-"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1tQ_BWFLvA-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training start\n",
      "Epoch 1 Iteration 0: Train Loss = 16.013107299804688\n",
      "Epoch 1 Iteration 330: Train Loss = 14.401381492614746\n",
      "Epoch 1 Iteration 660: Train Loss = 14.822908401489258\n",
      "Epoch 1 Iteration 990: Train Loss = 14.024886131286621\n",
      "Epoch 1 Iteration 1320: Train Loss = 12.64328384399414\n",
      "Epoch 1 Iteration 1650: Train Loss = 12.921857833862305\n",
      "Epoch 1 Iteration 1980: Train Loss = 12.911544799804688\n",
      "Epoch 1 Iteration 2310: Train Loss = 12.65061092376709\n",
      "Epoch 1 Iteration 2640: Train Loss = 11.84924030303955\n",
      "Epoch 1 Iteration 2970: Train Loss = 11.738849639892578\n",
      "Epoch 1 Iteration 3300: Train Loss = 12.773321151733398\n",
      "Epoch 1 Iteration 3630: Train Loss = 11.384119033813477\n",
      "Epoch 1 Iteration 3960: Train Loss = 12.204998970031738\n",
      "Epoch 1 Iteration 4290: Train Loss = 11.027189254760742\n",
      "End of epoch 1\n",
      "Epoch 1 training time cost 71.77 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [09:44<00:00,  2.08it/s]\n",
      "100%|██████████| 411/411 [03:02<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v24684491/.local/lib/python3.10/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (Precision@1) = 0.9475367395111551, (mAP) = 0.8640100383986594\n",
      "Epoch 1 test time cost 12.9 mins\n",
      "Epoch 2 training start\n",
      "Epoch 2 Iteration 0: Train Loss = 11.843191146850586\n",
      "Epoch 2 Iteration 330: Train Loss = 11.024640083312988\n",
      "Epoch 2 Iteration 660: Train Loss = 9.989233016967773\n",
      "Epoch 2 Iteration 990: Train Loss = 11.562463760375977\n",
      "Epoch 2 Iteration 1320: Train Loss = 10.903096199035645\n",
      "Epoch 2 Iteration 1650: Train Loss = 10.840314865112305\n",
      "Epoch 2 Iteration 1980: Train Loss = 9.962678909301758\n",
      "Epoch 2 Iteration 2310: Train Loss = 10.130949974060059\n",
      "Epoch 2 Iteration 2640: Train Loss = 10.49455738067627\n",
      "Epoch 2 Iteration 2970: Train Loss = 9.891399383544922\n",
      "Epoch 2 Iteration 3300: Train Loss = 10.073354721069336\n",
      "Epoch 2 Iteration 3630: Train Loss = 10.768989562988281\n",
      "Epoch 2 Iteration 3960: Train Loss = 9.64596176147461\n",
      "Epoch 2 Iteration 4290: Train Loss = 9.480974197387695\n",
      "End of epoch 2\n",
      "Epoch 2 training time cost 72.06 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [09:13<00:00,  2.19it/s]\n",
      "100%|██████████| 411/411 [03:13<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9526383918373562, (mAP) = 0.8774715451917586\n",
      "Epoch 2 test time cost 12.54 mins\n",
      "Epoch 3 training start\n",
      "Epoch 3 Iteration 0: Train Loss = 9.047769546508789\n",
      "Epoch 3 Iteration 330: Train Loss = 9.231979370117188\n",
      "Epoch 3 Iteration 660: Train Loss = 9.462691307067871\n",
      "Epoch 3 Iteration 990: Train Loss = 9.642138481140137\n",
      "Epoch 3 Iteration 1320: Train Loss = 9.108382225036621\n",
      "Epoch 3 Iteration 1650: Train Loss = 9.886247634887695\n",
      "Epoch 3 Iteration 1980: Train Loss = 9.715089797973633\n",
      "Epoch 3 Iteration 2310: Train Loss = 8.8630952835083\n",
      "Epoch 3 Iteration 2640: Train Loss = 9.286005973815918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Iteration 2970: Train Loss = 8.868804931640625\n",
      "Epoch 3 Iteration 3300: Train Loss = 9.285984992980957\n",
      "Epoch 3 Iteration 3630: Train Loss = 8.40355110168457\n",
      "Epoch 3 Iteration 3960: Train Loss = 8.600994110107422\n",
      "Epoch 3 Iteration 4290: Train Loss = 9.851364135742188\n",
      "End of epoch 3\n",
      "Epoch 3 training time cost 72.61 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [09:47<00:00,  2.07it/s]\n",
      "100%|██████████| 411/411 [02:59<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9540089849996193, (mAP) = 0.8784491147079492\n",
      "Epoch 3 test time cost 12.86 mins\n",
      "Epoch 4 training start\n",
      "Epoch 4 Iteration 0: Train Loss = 9.389657974243164\n",
      "Epoch 4 Iteration 330: Train Loss = 8.162039756774902\n",
      "Epoch 4 Iteration 660: Train Loss = 8.07362174987793\n",
      "Epoch 4 Iteration 990: Train Loss = 9.08753490447998\n",
      "Epoch 4 Iteration 1320: Train Loss = 9.282567024230957\n",
      "Epoch 4 Iteration 1650: Train Loss = 9.868605613708496\n",
      "Epoch 4 Iteration 1980: Train Loss = 8.436516761779785\n",
      "Epoch 4 Iteration 2310: Train Loss = 9.031244277954102\n",
      "Epoch 4 Iteration 2640: Train Loss = 7.921779632568359\n",
      "Epoch 4 Iteration 2970: Train Loss = 8.143840789794922\n",
      "Epoch 4 Iteration 3300: Train Loss = 8.60838794708252\n",
      "Epoch 4 Iteration 3630: Train Loss = 8.671504974365234\n",
      "Epoch 4 Iteration 3960: Train Loss = 8.186119079589844\n",
      "Epoch 4 Iteration 4290: Train Loss = 8.729968070983887\n",
      "End of epoch 4\n",
      "Epoch 4 training time cost 72.69 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [10:05<00:00,  2.00it/s]\n",
      "100%|██████████| 411/411 [03:03<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9557602984847331, (mAP) = 0.8822234460723638\n",
      "Epoch 4 test time cost 13.23 mins\n",
      "Epoch 5 training start\n",
      "Epoch 5 Iteration 0: Train Loss = 8.118392944335938\n",
      "Epoch 5 Iteration 330: Train Loss = 7.804272174835205\n",
      "Epoch 5 Iteration 660: Train Loss = 9.203898429870605\n",
      "Epoch 5 Iteration 990: Train Loss = 7.9780144691467285\n",
      "Epoch 5 Iteration 1320: Train Loss = 8.133490562438965\n",
      "Epoch 5 Iteration 1650: Train Loss = 7.792478084564209\n",
      "Epoch 5 Iteration 1980: Train Loss = 6.937102794647217\n",
      "Epoch 5 Iteration 2310: Train Loss = 7.241811752319336\n",
      "Epoch 5 Iteration 2640: Train Loss = 7.638871669769287\n",
      "Epoch 5 Iteration 2970: Train Loss = 8.572898864746094\n",
      "Epoch 5 Iteration 3300: Train Loss = 8.11917781829834\n",
      "Epoch 5 Iteration 3630: Train Loss = 8.459342002868652\n",
      "Epoch 5 Iteration 3960: Train Loss = 7.769558906555176\n",
      "Epoch 5 Iteration 4290: Train Loss = 7.560789585113525\n",
      "End of epoch 5\n",
      "Epoch 5 training time cost 72.97 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [08:55<00:00,  2.27it/s]\n",
      "100%|██████████| 411/411 [03:07<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9549227137744613, (mAP) = 0.8780393173556224\n",
      "Epoch 5 test time cost 12.13 mins\n",
      "Epoch 6 training start\n",
      "Epoch 6 Iteration 0: Train Loss = 7.5110297203063965\n",
      "Epoch 6 Iteration 330: Train Loss = 7.689380168914795\n",
      "Epoch 6 Iteration 660: Train Loss = 6.572943687438965\n",
      "Epoch 6 Iteration 990: Train Loss = 7.5818376541137695\n",
      "Epoch 6 Iteration 1320: Train Loss = 7.613896369934082\n",
      "Epoch 6 Iteration 1650: Train Loss = 6.724954128265381\n",
      "Epoch 6 Iteration 1980: Train Loss = 7.959171772003174\n",
      "Epoch 6 Iteration 2310: Train Loss = 7.767221927642822\n",
      "Epoch 6 Iteration 2640: Train Loss = 7.4252028465271\n",
      "Epoch 6 Iteration 2970: Train Loss = 8.406665802001953\n",
      "Epoch 6 Iteration 3300: Train Loss = 7.206853866577148\n",
      "Epoch 6 Iteration 3630: Train Loss = 7.717713356018066\n",
      "Epoch 6 Iteration 3960: Train Loss = 7.570240020751953\n",
      "Epoch 6 Iteration 4290: Train Loss = 6.5369744300842285\n",
      "End of epoch 6\n",
      "Epoch 6 training time cost 72.87 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [09:03<00:00,  2.23it/s]\n",
      "100%|██████████| 411/411 [03:28<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9573593238407065, (mAP) = 0.8857916262763188\n",
      "Epoch 6 test time cost 12.6 mins\n",
      "Epoch 7 training start\n",
      "Epoch 7 Iteration 0: Train Loss = 7.051388740539551\n",
      "Epoch 7 Iteration 330: Train Loss = 6.490564823150635\n",
      "Epoch 7 Iteration 660: Train Loss = 6.88668966293335\n",
      "Epoch 7 Iteration 990: Train Loss = 8.113783836364746\n",
      "Epoch 7 Iteration 1320: Train Loss = 6.950926780700684\n",
      "Epoch 7 Iteration 1650: Train Loss = 6.798108100891113\n",
      "Epoch 7 Iteration 1980: Train Loss = 7.163785934448242\n",
      "Epoch 7 Iteration 2310: Train Loss = 7.413910865783691\n",
      "Epoch 7 Iteration 2640: Train Loss = 6.739411354064941\n",
      "Epoch 7 Iteration 2970: Train Loss = 6.486433029174805\n",
      "Epoch 7 Iteration 3300: Train Loss = 7.657275199890137\n",
      "Epoch 7 Iteration 3630: Train Loss = 8.076608657836914\n",
      "Epoch 7 Iteration 3960: Train Loss = 6.154167175292969\n",
      "Epoch 7 Iteration 4290: Train Loss = 7.914617538452148\n",
      "End of epoch 7\n",
      "Epoch 7 training time cost 72.97 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [08:54<00:00,  2.27it/s]\n",
      "100%|██████████| 411/411 [03:16<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9562171628721541, (mAP) = 0.8799269997997387\n",
      "Epoch 7 test time cost 12.27 mins\n",
      "Epoch 8 training start\n",
      "Epoch 8 Iteration 0: Train Loss = 7.182656764984131\n",
      "Epoch 8 Iteration 330: Train Loss = 6.332995891571045\n",
      "Epoch 8 Iteration 660: Train Loss = 6.446353435516357\n",
      "Epoch 8 Iteration 990: Train Loss = 6.645884990692139\n",
      "Epoch 8 Iteration 1320: Train Loss = 6.956620216369629\n",
      "Epoch 8 Iteration 1650: Train Loss = 7.629325866699219\n",
      "Epoch 8 Iteration 1980: Train Loss = 6.086648464202881\n",
      "Epoch 8 Iteration 2310: Train Loss = 7.07792854309082\n",
      "Epoch 8 Iteration 2640: Train Loss = 6.672582149505615\n",
      "Epoch 8 Iteration 2970: Train Loss = 6.807537078857422\n",
      "Epoch 8 Iteration 3300: Train Loss = 7.160943031311035\n",
      "Epoch 8 Iteration 3630: Train Loss = 6.568235397338867\n",
      "Epoch 8 Iteration 3960: Train Loss = 6.731245994567871\n",
      "Epoch 8 Iteration 4290: Train Loss = 6.301626682281494\n",
      "End of epoch 8\n",
      "Epoch 8 training time cost 72.88 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [08:52<00:00,  2.28it/s]\n",
      "100%|██████████| 411/411 [03:06<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9549227137744613, (mAP) = 0.8748594554287081\n",
      "Epoch 8 test time cost 12.07 mins\n",
      "Epoch 9 training start\n",
      "Epoch 9 Iteration 0: Train Loss = 5.649981498718262\n",
      "Epoch 9 Iteration 330: Train Loss = 6.37014627456665\n",
      "Epoch 9 Iteration 660: Train Loss = 6.64243221282959\n",
      "Epoch 9 Iteration 990: Train Loss = 6.462119102478027\n",
      "Epoch 9 Iteration 1320: Train Loss = 6.171957969665527\n",
      "Epoch 9 Iteration 1650: Train Loss = 7.107519149780273\n",
      "Epoch 9 Iteration 1980: Train Loss = 6.850185871124268\n",
      "Epoch 9 Iteration 2310: Train Loss = 6.693580627441406\n",
      "Epoch 9 Iteration 2640: Train Loss = 6.2504167556762695\n",
      "Epoch 9 Iteration 2970: Train Loss = 6.7143635749816895\n",
      "Epoch 9 Iteration 3300: Train Loss = 6.30639123916626\n",
      "Epoch 9 Iteration 3630: Train Loss = 6.727115631103516\n",
      "Epoch 9 Iteration 3960: Train Loss = 8.225317001342773\n",
      "Epoch 9 Iteration 4290: Train Loss = 6.892694473266602\n",
      "End of epoch 9\n",
      "Epoch 9 training time cost 72.78 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [11:29<00:00,  1.76it/s]\n",
      " 12%|█▏        | 50/411 [00:30<03:19,  1.81it/s]"
     ]
    }
   ],
   "source": [
    "# tmp = torch.load(\"/work/v24684491/Saved_models/Swinv2_Arc_1217_only_400384/Swinv2_Arc8.pth\")\n",
    "# model.load_state_dict(tmp['state_dict'])\n",
    "# optimizer.load_state_dict(tmp['optimizer_state_dict'])\n",
    "# metric.load_state_dict(tmp['metric_dict'])\n",
    "# metric_optimizer.load_state_dict(tmp['metric_optimizer_dict'])\n",
    "# optimizer.param_groups[0]['lr'] = lr=1e-4\n",
    "# metric_optimizer.param_groups[0]['lr'] = 5e-3\n",
    "# model = nn.DataParallel(model)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs )\n",
    "# metric_scheduler = CosineAnnealingLR(metric_optimizer, T_max=num_epochs )\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer , step_size = 5,gamma= 0.1, last_epoch= -1)\n",
    "# metric_scheduler = torch.optim.lr_scheduler.StepLR(metric_optimizer , step_size = 5,gamma= 0.1, last_epoch= -1)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    time_start = time.time()\n",
    "#     train(model, loss_func, device, train_loader, optimizer, loss_optimizer, epoch)\n",
    "    train(model, metric, loss_func, device, train_loader, optimizer, metric_optimizer, epoch, scheduler, metric_scheduler)\n",
    "#     train(model, metric, loss_func, device, train_loader, optimizer, metric_optimizer, epoch)\n",
    "    \n",
    "    # 古くて低い性能のモデルは，こまめに削除してディスク容量対策する\n",
    "    torch.save({'state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            \"metric_optimizer_dict\" : metric_optimizer.state_dict(),\n",
    "            \"metric_dict\" : metric.state_dict(),\n",
    "            \"lr_scheduler_dict\" : scheduler.state_dict(),\n",
    "            \"metric_lr_scheduler_dict\" : metric_scheduler.state_dict(),\n",
    "            'epochs' : epoch }, file_name +'Swinv2_Arc{}.pth'.format(epoch))\n",
    "    \n",
    "    # torch.save({\"state_dict\":model.state_dict(),\"epoch\":epoch}, file_name +'EffArc_Gem-test_epoch{}.pth'.format(epoch))\n",
    "    \n",
    "    time_end = time.time()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    print(f'Epoch {epoch} training time cost {round(time_c/60,2)} mins') \n",
    "    \n",
    "    test(test_db_data, test_query_data, model, accuracy_calculator)\n",
    "\n",
    "    val_time_end = time.time()    #結束計時\n",
    "    time_c= val_time_end - time_end   #執行所花時間\n",
    "    print(f'Epoch {epoch} test time cost {round(time_c/60,2)} mins')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SubCenterArcFaceMNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
